{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fd6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b12b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "raw_csv_data = np.loadtxt('./Audiobooks_data.csv', delimiter=',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:, 1:-1]\n",
    "targets_all = raw_csv_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb843b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "zero_targets_counter = 0\n",
    "\n",
    "indices_to_remove = []\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0673f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7262ba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788.0 3579 0.49958088851634536\n",
      "232.0 447 0.5190156599552572\n",
      "217.0 448 0.484375\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed068fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('valid', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7139358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('train.npz')\n",
    "\n",
    "train_input = npz['inputs'].astype(np.float32)\n",
    "train_traget = npz['targets'].astype(np.float32)\n",
    "\n",
    "npz = np.load('valid.npz')\n",
    "valid_input = npz['inputs'].astype(np.float32)\n",
    "valid_target = npz['targets'].astype(np.float32)\n",
    "\n",
    "npz = np.load('test.npz')\n",
    "test_input = npz['inputs'].astype(np.float32)\n",
    "test_target = npz['targets'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae164a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726fac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - 34ms/step - accuracy: 0.4979 - loss: 0.7523 - val_accuracy: 0.6130 - val_loss: 0.6831\n",
      "Epoch 2/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.7279 - loss: 0.6273 - val_accuracy: 0.7830 - val_loss: 0.5980\n",
      "Epoch 3/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8145 - loss: 0.5326 - val_accuracy: 0.8523 - val_loss: 0.4935\n",
      "Epoch 4/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8497 - loss: 0.4381 - val_accuracy: 0.8747 - val_loss: 0.3967\n",
      "Epoch 5/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8653 - loss: 0.3718 - val_accuracy: 0.8837 - val_loss: 0.3367\n",
      "Epoch 6/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8757 - loss: 0.3356 - val_accuracy: 0.8881 - val_loss: 0.3068\n",
      "Epoch 7/100\n",
      "36/36 - 0s - 5ms/step - accuracy: 0.8801 - loss: 0.3160 - val_accuracy: 0.8926 - val_loss: 0.2908\n",
      "Epoch 8/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8854 - loss: 0.3028 - val_accuracy: 0.8971 - val_loss: 0.2823\n",
      "Epoch 9/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8888 - loss: 0.2933 - val_accuracy: 0.8971 - val_loss: 0.2725\n",
      "Epoch 10/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8913 - loss: 0.2849 - val_accuracy: 0.8949 - val_loss: 0.2718\n",
      "Epoch 11/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8961 - loss: 0.2781 - val_accuracy: 0.8971 - val_loss: 0.2678\n",
      "Epoch 12/100\n",
      "36/36 - 0s - 3ms/step - accuracy: 0.8969 - loss: 0.2727 - val_accuracy: 0.8971 - val_loss: 0.2626\n",
      "Epoch 13/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8997 - loss: 0.2685 - val_accuracy: 0.8993 - val_loss: 0.2618\n",
      "Epoch 14/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.8986 - loss: 0.2644 - val_accuracy: 0.8993 - val_loss: 0.2601\n",
      "Epoch 15/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9039 - loss: 0.2609 - val_accuracy: 0.9016 - val_loss: 0.2584\n",
      "Epoch 16/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9042 - loss: 0.2571 - val_accuracy: 0.9016 - val_loss: 0.2555\n",
      "Epoch 17/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9033 - loss: 0.2543 - val_accuracy: 0.9016 - val_loss: 0.2546\n",
      "Epoch 18/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9044 - loss: 0.2520 - val_accuracy: 0.9038 - val_loss: 0.2547\n",
      "Epoch 19/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9042 - loss: 0.2495 - val_accuracy: 0.9016 - val_loss: 0.2524\n",
      "Epoch 20/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9047 - loss: 0.2481 - val_accuracy: 0.9038 - val_loss: 0.2530\n",
      "Epoch 21/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9078 - loss: 0.2461 - val_accuracy: 0.9016 - val_loss: 0.2510\n",
      "Epoch 22/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9070 - loss: 0.2448 - val_accuracy: 0.9038 - val_loss: 0.2483\n",
      "Epoch 23/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9067 - loss: 0.2422 - val_accuracy: 0.9038 - val_loss: 0.2493\n",
      "Epoch 24/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9081 - loss: 0.2423 - val_accuracy: 0.9038 - val_loss: 0.2491\n",
      "Epoch 25/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9092 - loss: 0.2408 - val_accuracy: 0.9038 - val_loss: 0.2461\n",
      "Epoch 26/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9092 - loss: 0.2386 - val_accuracy: 0.9038 - val_loss: 0.2475\n",
      "Epoch 27/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9089 - loss: 0.2380 - val_accuracy: 0.9060 - val_loss: 0.2454\n",
      "Epoch 28/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9095 - loss: 0.2366 - val_accuracy: 0.9060 - val_loss: 0.2438\n",
      "Epoch 29/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9100 - loss: 0.2355 - val_accuracy: 0.9038 - val_loss: 0.2428\n",
      "Epoch 30/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9100 - loss: 0.2348 - val_accuracy: 0.9038 - val_loss: 0.2458\n",
      "Epoch 31/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9095 - loss: 0.2346 - val_accuracy: 0.9060 - val_loss: 0.2459\n",
      "Epoch 32/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9123 - loss: 0.2334 - val_accuracy: 0.9038 - val_loss: 0.2429\n",
      "Epoch 33/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9125 - loss: 0.2331 - val_accuracy: 0.9038 - val_loss: 0.2395\n",
      "Epoch 34/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9128 - loss: 0.2322 - val_accuracy: 0.9016 - val_loss: 0.2428\n",
      "Epoch 35/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9123 - loss: 0.2318 - val_accuracy: 0.9060 - val_loss: 0.2375\n",
      "Epoch 36/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9125 - loss: 0.2313 - val_accuracy: 0.9060 - val_loss: 0.2400\n",
      "Epoch 37/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9125 - loss: 0.2299 - val_accuracy: 0.9060 - val_loss: 0.2381\n",
      "Epoch 38/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9125 - loss: 0.2301 - val_accuracy: 0.9060 - val_loss: 0.2404\n",
      "Epoch 39/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9134 - loss: 0.2294 - val_accuracy: 0.9038 - val_loss: 0.2437\n",
      "Epoch 40/100\n",
      "36/36 - 0s - 4ms/step - accuracy: 0.9148 - loss: 0.2292 - val_accuracy: 0.9038 - val_loss: 0.2418\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 35.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21948606480>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "\n",
    "earlt_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(train_input,train_targets,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          validation_data=(valid_input, valid_target),\n",
    "          callbacks=[earlt_stopping],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a4bf913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9257 - loss: 0.2312 \n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy = model.evaluate(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba6224d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m customer_features \u001b[38;5;241m=\u001b[39m customer_row[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Scale features (using the same method as training)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m customer_features_scaled \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mscale(customer_features)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     22\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(customer_features_scaled)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the customer ID you want to predict\n",
    "customer_id_to_predict = 25164 # Replace with the actual customer ID\n",
    "\n",
    "# Load the original data\n",
    "raw_csv_data = np.loadtxt('./Audiobooks_data.csv', delimiter=',')\n",
    "\n",
    "# Find the row with the matching customer ID\n",
    "customer_row = raw_csv_data[raw_csv_data[:, 0] == customer_id_to_predict]\n",
    "\n",
    "if customer_row.shape[0] == 0:\n",
    "    print(\"Customer ID not found.\")\n",
    "else:\n",
    "    # Extract features (columns 1 to -1, as in your training)\n",
    "    customer_features = customer_row[:, 1:-1]\n",
    "\n",
    "    # Scale features (using the same method as training)\n",
    "    customer_features_scaled = preprocessing.scale(customer_features)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(customer_features_scaled)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    print(f\"Predicted class for customer {customer_id_to_predict}: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7773707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load the original data\n",
    "# raw_csv_data = np.loadtxt('./Audiobooks_data.csv', delimiter=',')\n",
    "\n",
    "# # Get all customer IDs and features\n",
    "# customer_ids = raw_csv_data[:, 0]\n",
    "# customer_features = raw_csv_data[:, 1:-1]\n",
    "\n",
    "# # Scale features (using the same method as training)\n",
    "# customer_features_scaled = preprocessing.scale(customer_features)\n",
    "\n",
    "# # Predict for all customers\n",
    "# predictions = model.predict(customer_features_scaled)\n",
    "# predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Print customer IDs with their predicted class\n",
    "# for idx, pred_class in enumerate(predicted_classes):\n",
    "#     print(f\"Customer ID: {int(customer_ids[idx])} - Predicted class: {pred_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261ccb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d85ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a2310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
